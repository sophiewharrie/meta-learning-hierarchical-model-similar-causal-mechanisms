name: Experiments

# NOTE this conda environment is for the competitive baselines, 
# which is different to the environment for BNN baselines or our methods 
# (which just use the local virtual env)
conda_env: baselines/environments/env.yml

entry_points:
  # entry point for creating synthetic datasets
  create_synthetic_dataset:
      parameters:
          outprefix: # prefix for the output
            type: string
            default: data/synthetic/output/test
          seed: # random seed for reproducibility
            type: float
            default: 1234
          N_train: # number of training tasks
            type: int
            default: 100
          N_val: # number of validation tasks
            type: int
            default: 30
          N_test: # number of test tasks
            type: int
            default: 30
          M_train: # number of training (support) samples per task
            type: int
            default: 40
          M_test: # numner of test (query) samples per task
            type: int
            default: 40
          C: # number of causal clusters
            type: float
            default: 6
          sigma_ref: # variance of coefficient distributions in (globally)
            type: float
            default: 1.0
          sigma_group: # variance of coefficient distributions in for a causal group
            type: float
            default: 0.02
          sigma_task: # variance of coefficient distributions in for a single task
            type: float
            default: 0.0001
          sigma_noise: # variance of noise distributions in
            type: float
            default: 0.1
          eta_group: # bound on divergence from the reference DAG across the causal groups
            type: float
            default: 0.5
          eta_task: # bound on divergence from the reference DAG within a causal group
            type: float
            default: 0.001
          p_interv: # ratio of observational to interventional data (p=0.3 means 30% observational data, 70% interventional data)
            type: float
            default: 0.3
      command: "python3 ./data_generator/main.py --outprefix {outprefix} --seed {seed} --N_train {N_train} --N_val {N_val} --N_test {N_test} --M_train {M_train} --M_test {M_test} --C {C} --sigma_ref {sigma_ref} --sigma_group {sigma_group} --sigma_task {sigma_task} --sigma_noise {sigma_noise} --eta_group {eta_group} --eta_task {eta_task} --p_interv {p_interv} --linear"
    # entry point for our method
  our_method_unknown_causal_structure:
      parameters:
          datafile: # path for the dataset (.csv)
            type: string
            default: data/synthetic/output/test_data.csv
          intervfile: # path for intervention mask (.csv)
            type: string
            default: data/synthetic/output/test_intervmask.csv # put ignore if not using
          causal_distance: # inferred from data for unknown causal models
            type: string
            default: inferred
          num_groups: # number of causal groups
            type: float
            default: 6
          outprefix: # prefix for the output
            type: string
            default: data/synthetic/output/test
          task: # either regression or classification
            type: string
            default: regression
          num_pretrain_epochs: # number of updates for causal model
            type: float
            default: 500
          num_initial_epochs: # number of epochs for initialisation
            type: float
            default: 10
          num_main_epochs: # (maximum) number of training epochs for main predictive model
            type: float
            default: 100
          patience: # how many increases in validation loss to go before early stopping of training for main model (to prevent overfitting)
            type: float
            default: 3
          meta_learning_rate_global: # learning rate for global (meta) model (used in meta-training)
            type: float
            default: 0.001
          meta_learning_rate_group: # learning rate for group (meta) model (used in meta-training)
            type: float
            default: 0.0001
          base_learning_rate: # 'learning rate for inner (base) model (used in both meta-training and meta-testing)
            type: float
            default: 0.001
          hidden_layer_size: # dimension of hidden layers (assumes 2 hidden layers)
            type: float
            default: 20
          num_base_steps: # number of SGD steps for base learner
            type: float
            default: 2
          n_edges_per_node: # prior for number of edges per node in causal DAG
            type: float
            default: 2
          obs_noise: # prior noise for causal likelihood model (gaussian SCM)
            type: float
            default: 0.1
          alpha: # initial value of inverse temperature for p(G|Z)
            type: float
            default: 0.02
          prior_pi: # pi on the scaled mixture prior
            type: float
            default: 1.0
          prior_sigma_1: # prior sigma 1 on the mixture prior distribution
            type: float
            default: 0.1
          prior_sigma_2: # prior sigma 2 on the mixture prior distribution
            type: float
            default: 0.01
          lambda: # pi on the scaled mixture prior
            type: float
            default: 0.01
          causal_prior: # which prior to use
            type: string
            default: erdos_renyi # erdos_renyi or scale_free
          causal_model: # which causal model to use
            type: string
            default: linear # linear or non_linear
          bandwidth_z: # bandwidth parameter for kernel of Z
            type: float
            default: 5
          bandwidth_theta: # bandwidth parameter for kernel of Theta
            type: float
            default: 1000
      command: "python3 model/main.py --datafile {datafile} --intervfile {intervfile} --causal_distance {causal_distance} --num_groups {num_groups} --outprefix {outprefix} --task {task} --num_pretrain_epochs {num_pretrain_epochs} --num_initial_epochs {num_initial_epochs} --num_main_epochs {num_main_epochs} --patience {patience} --meta_learning_rate_global {meta_learning_rate_global} --meta_learning_rate_group {meta_learning_rate_group} --base_learning_rate {base_learning_rate} --hidden_layer_size {hidden_layer_size} --num_base_steps {num_base_steps} --n_edges_per_node {n_edges_per_node} --obs_noise {obs_noise} --alpha {alpha} --prior_pi {prior_pi} --prior_sigma_1 {prior_sigma_1} --prior_sigma_2 {prior_sigma_2} --lambda {lambda} --causal_prior {causal_prior} --causal_model {causal_model} --bandwidth_z {bandwidth_z} --bandwidth_theta {bandwidth_theta}"
  # entry point for running a global BNN model (complete pooling)
  global_bnn_model:
      parameters:
          datafile: # path for the dataset (.csv)
            type: string
            default: data/synthetic/output/test_data.csv
          outprefix: # prefix for the output
            type: string
            default: data/synthetic/output/test
          task: # either regression or classification
            type: string
            default: regression
          inference: # either HMC or SVI
            type: string
            default: SVI
          num_samples: # number of MC samples to generate
            type: float
            default: 1000
          hidden_layer_size: # dimension of hidden layers (assumes 2 hidden layers)
            type: float
            default: 20
          learning_rate: # learning rate for model training (uses Adam optimiser)
            type: float
            default: 0.001
          num_steps: # number of steps for model training
            type: float
            default: 10000
      command: "python3 ./baselines/bnn_baselines/global_model_main.py --datafile {datafile} --outprefix {outprefix} --task {task} --inference {inference} --num_samples {num_samples} --hidden_layer_size {hidden_layer_size} --learning_rate {learning_rate} --num_steps {num_steps}"
  # entry point for running a larger global BNN model (complete pooling) with 2x the number of parameters
  global_bnn_model_large:
      parameters:
          datafile: # path for the dataset (.csv)
            type: string
            default: data/synthetic/output/test_data.csv
          outprefix: # prefix for the output
            type: string
            default: data/synthetic/output/test
          task: # either regression or classification
            type: string
            default: regression
          inference: # either HMC or SVI
            type: string
            default: SVI
          num_samples: # number of MC samples to generate
            type: float
            default: 1000
          hidden_layer_size: # dimension of hidden layers (assumes 2 hidden layers)
            type: float
            default: 40
          learning_rate: # learning rate for model training (uses Adam optimiser)
            type: float
            default: 0.001
          num_steps: # number of steps for model training
            type: float
            default: 10000
      command: "python3 ./baselines/bnn_baselines/global_model_main.py --datafile {datafile} --outprefix {outprefix} --task {task} --inference {inference} --num_samples {num_samples} --hidden_layer_size {hidden_layer_size} --learning_rate {learning_rate} --num_steps {num_steps} --large_model"
  # entry point for running an individual BNN model (no pooling)
  individual_bnn_model:
      parameters:
          datafile: # path for the dataset (.csv)
            type: string
            default: data/synthetic/output/test_data.csv
          outprefix: # prefix for the output
            type: string
            default: data/synthetic/output/test
          task: # either regression or classification
            type: string
            default: regression
          inference: # either HMC or SVI
            type: string
            default: SVI
          num_samples: # number of MC samples to generate
            type: float
            default: 1000
          hidden_layer_size: # dimension of hidden layers (assumes 2 hidden layers)
            type: float
            default: 20
          learning_rate: # learning rate for model training (uses Adam optimiser)
            type: float
            default: 0.001
          num_steps: # number of steps for model training
            type: float
            default: 10000
      command: "python3 ./baselines/bnn_baselines/individual_model_main.py --datafile {datafile} --outprefix {outprefix} --task {task} --inference {inference} --num_samples {num_samples} --hidden_layer_size {hidden_layer_size} --learning_rate {learning_rate} --num_steps {num_steps}"
  # entry point for running the HSML model (from https://github.com/huaxiuyao/HSML)
  hsml_model:
      parameters:
          datafile: # path for the dataset (.csv)
            type: string
            default: data/synthetic/output/test_data.csv
          outdir: # directory for the output, e.g. /baselines/HSML/logs/test/
            type: string
            default: baselines/HSML/logs/test/
          hidden_layer_size: # dimension of hidden layers (assumes 2 hidden layers)
            type: float
            default: 20
          metatrain_iterations: # number of training epochs
            type: float
            default: 100
          meta_lr: # number of training epochs
            type: float
            default: 0.001
          update_lr: # step size alpha for inner gradient update
            type: float
            default: 0.001
          num_updates: # number of inner gradient updates during training
            type: float
            default: 2
      command: "python baselines/HSML/custom_main.py --datafile {datafile} --outdir {outdir} --hidden_layer_size {hidden_layer_size} --metatrain_iterations {metatrain_iterations} --meta_lr {meta_lr} --update_lr {update_lr} --num_updates {num_updates}"
  # entry point for running the TSA MAML model (from https://github.com/Carbonaraa/TSA-MAML)
  tsa_maml_model:
      parameters:
          datafile: # path for the dataset (.csv)
            type: string
            default: data/synthetic/output/test_data.csv
          outprefix: # prefix for the output
            type: string
            default: data/synthetic/output/test
          num_groups: # number of causal groups
            type: float
            default: 2
          task: # either regression or classification
            type: string
            default: regression
          hidden_layer_size: # dimension of hidden layers (assumes 2 hidden layers)
            type: float
            default: 20
          num_initial_epochs: # number of training epochs for MAML model (initialisation)
            type: float
            default: 50
          num_main_epochs: # max number of training epochs for TSA-MAML model (main)
            type: float
            default: 100
          meta_learning_rate_initial: # learning rate for outer (meta) MAML model (initialisation)
            type: float
            default: 0.001
          base_learning_rate_initial: # learning rate for inner (base) MAML model (initialisation)
            type: float
            default: 0.001
          meta_learning_rate_main: # learning rate for outer (meta) TSA-MAML model (main)
            type: float
            default: 0.001
          base_learning_rate_main: # learning rate for inner (base) TSA-MAML model (main)
            type: float
            default: 0.001
          num_base_steps: # number of SGD steps for base learner
            type: float
            default: 2
      command: "python3 baselines/TSA-MAML/our_tsa_maml.py --datafile {datafile} --outprefix {outprefix} --num_groups {num_groups} --task {task} --hidden_layer_size {hidden_layer_size} --num_initial_epochs {num_initial_epochs} --num_main_epochs {num_main_epochs} --meta_learning_rate_initial {meta_learning_rate_initial} --base_learning_rate_initial {base_learning_rate_initial} --meta_learning_rate_main {meta_learning_rate_main} --base_learning_rate_main {base_learning_rate_main} --num_base_steps {num_base_steps}"
  # entry point for running the Bayesian meta-learning baseline model
  bayesian_metalearner_model:
      parameters:
          datafile: # path for the dataset (.csv)
            type: string
            default: data/synthetic/output/test_data.csv
          outprefix: # prefix for the output
            type: string
            default: data/synthetic/output/test
          task: # either regression or classification
            type: string
            default: regression
          hidden_layer_size: # dimension of hidden layers (assumes 2 hidden layers)
            type: float
            default: 20
          num_epochs: # max number of training epochs
            type: float
            default: 100
          meta_learning_rate: # learning rate for outer (meta) model (used in meta-training)
            type: float
            default: 0.001
          base_learning_rate: # learning rate for inner (base) model (used in both meta-training and meta-testing)
            type: float
            default: 0.0001
          num_base_steps: # number of SGD steps for base learner
            type: float
            default: 2
          prior_sigma_1: # prior sigma 1 on the mixture prior distribution
            type: float
            default: 0.1
          prior_sigma_2: # prior sigma 2 on the mixture prior distribution
            type: float
            default: 0.4
          prior_pi: # pi on the scaled mixture prior
            type: float
            default: 1.0
      command: "python3 baselines/bayesian_metalearner/bayesian_metalearner_main.py --datafile {datafile} --outprefix {outprefix} --task {task} --hidden_layer_size {hidden_layer_size} --num_epochs {num_epochs} --meta_learning_rate {meta_learning_rate} --base_learning_rate {base_learning_rate} --num_base_steps {num_base_steps} --prior_sigma_1 {prior_sigma_1} --prior_sigma_2 {prior_sigma_2} --prior_pi {prior_pi}"