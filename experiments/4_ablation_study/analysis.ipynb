{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare both generalisability and accuracy at recovering true causal groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "import datetime\n",
    "\n",
    "experiment_id = '625001992703083237' # TODO update ID if it changes\n",
    "\n",
    "mlflow.set_tracking_uri(\"../../mlruns\")\n",
    "\n",
    "runs = mlflow.search_runs(experiment_ids=[experiment_id])\n",
    "\n",
    "failed_runs = len(runs[runs['status']=='FAILED'][['params.model']])\n",
    "print(\"{} experiment runs failed ({}% of total)\".format(failed_runs, failed_runs/len(runs)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "runs.to_csv(f'results-main-{timestamp}.csv', index=None)\n",
    "print(timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = runs[['metrics.RMSE_avg_test','metrics.RMSE_avg_val','metrics.total_num_epochs','metrics.runtime_initialisation','metrics.runtime_main_training','params.outprefix','params.datafile']]\n",
    "results.insert(0, 'trial', results['params.outprefix'].str.split('/').str[-1].str.split('_').str[5])\n",
    "results.insert(0, 'dataset', results['params.outprefix'].str.split('/').str[-1].str.split('_').str[2])\n",
    "results.insert(0, 'model', results['params.outprefix'].str.split('/').str[-1].str.split('_').str[6])\n",
    "results = results[~results['metrics.RMSE_avg_test'].isna()]\n",
    "results.sort_values(by='metrics.RMSE_avg_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results = results[['trial','model','dataset','metrics.RMSE_avg_test', 'metrics.RMSE_avg_val']].groupby(['model','dataset']).mean()\n",
    "final_results.sort_values(by=['dataset','metrics.RMSE_avg_test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from f1_score_utils import get_f1_scores\n",
    "\n",
    "results['casualassignment_final_path'] = '../../' + results['params.outprefix'] + '_causal_assignments_final.csv'\n",
    "results['groundtruth_path'] = results['params.datafile'].str[0:-8] + 'task_metadata.csv'\n",
    "\n",
    "f1_final_train_list = []\n",
    "f1_final_val_test_list = []\n",
    "\n",
    "for idx, row in results.iterrows():\n",
    "\n",
    "    if row['model'] not in ['metalearner','tsamaml','ourmethodknowncausalmodels']:\n",
    "        f1_final_train, f1_final_val_test = get_f1_scores(row['casualassignment_final_path'], row['groundtruth_path'], 200)\n",
    "    else:\n",
    "        f1_final_train, f1_final_val_test = None, None\n",
    "\n",
    "    f1_final_train_list.append(f1_final_train)\n",
    "    f1_final_val_test_list.append(f1_final_val_test)\n",
    "\n",
    "results['f1_final_train'] = f1_final_train_list\n",
    "results['f1_final_val_test'] = f1_final_val_test_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.to_csv(f'results-summary-{timestamp}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = results[['dataset','trial','model', 'metrics.RMSE_avg_test', 'f1_final_val_test', 'f1_final_train']].groupby(['dataset','model'])['metrics.RMSE_avg_test', 'f1_final_val_test', 'f1_final_train'].agg(['mean', 'sem']).reset_index()\n",
    "for metric in ['metrics.RMSE_avg_test', 'f1_final_val_test', 'f1_final_train']:\n",
    "    tmp[\"{}_fmt\".format(metric)] = tmp.apply(lambda x: f\"{x[(metric,'mean')]:.4f} ({x[(metric,'sem')]:.3f})\", axis=1)\n",
    "    tmp = tmp.drop(columns=[(metric,'mean'),(metric,'sem')])\n",
    "\n",
    "\n",
    "order_map = {'ourmethodunknowncausalmodels':0, 'ourmethodunknowncausalmodelsnolatent':1, 'ourmethodunknowncausalmodelsnocausal':2, 'metalearner':3, 'ourmethodunknowncausalmodelsnoglobal':4}\n",
    "tmp = tmp[tmp['model'].isin(order_map.keys())]\n",
    "tmp['method_order'] = tmp['model'].map(order_map)\n",
    "for dataset in ['C','M','intervp']:\n",
    "    tmp[tmp['dataset']==dataset].sort_values(by=['dataset','method_order'])[['metrics.RMSE_avg_test_fmt','f1_final_train_fmt','f1_final_val_test_fmt']].to_csv(f'table_for_publication_{dataset}.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base_ml",
   "language": "python",
   "name": "base_ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
